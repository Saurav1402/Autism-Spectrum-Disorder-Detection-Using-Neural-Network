{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35c995cc-b371-4330-a4e6-e627201fbbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from PIL import Image, ImageTk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd5426bd-1b35-4d04-bb74-73fe1146b167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset path\n",
    "dataset_path = r\"C:\\Users\\hp\\OneDrive\\Desktop\\project research paper\\project implementation\\facial data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c2254fa-6a8a-4755-aab1-5a962c7fe4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories (labels)\n",
    "categories = [\"Autistic\", \"Non_Autistic\"]\n",
    "img_size = 128  # Resize images to 128x128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "763cf71c-9668-4349-b30b-5d471fc89746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    data = []\n",
    "    labels = []\n",
    "    for category in categories:\n",
    "        path = os.path.join(dataset_path, category)\n",
    "        label = categories.index(category)  # 0 for autistic, 1 for non-autistic\n",
    "        for img_name in os.listdir(path):\n",
    "            img_path = os.path.join(path, img_name)\n",
    "            try:\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "                img = cv2.resize(img, (img_size, img_size))\n",
    "                data.append(img)\n",
    "                labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_path}: {e}\")\n",
    "    return np.array(data), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9473a7a0-524c-4962-9635-9ab4c3dd07f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data, labels = load_dataset()\n",
    "data = data / 255.0  # Normalize pixel values (0-1)\n",
    "labels_numeric = np.array(labels)  # Convert labels to categorical format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "106983f9-3ecf-45eb-a09e-96ace36bde0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights to handle imbalance\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=labels_numeric)\n",
    "class_weights = {i: class_weights[i] for i in range(len(categories))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5293543-2bb6-4104-8c7a-5cab3d2de0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoding\n",
    "labels = keras.utils.to_categorical(labels_numeric, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07deb673-cce7-4052-9e62-2a656d2c8db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f313ffe-159d-42c2-9070-727dd558a34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "data_gen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "299586a4-b9d2-41eb-8932-0a31fdbb8453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Build Improved CNN Model\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_size, img_size, 3)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(2, activation='softmax')  # 2 output classes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1c96632-f9e9-4e9c-8c11-b0155a47473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    return lr * 0.95 if epoch > 5 else lr\n",
    "\n",
    "lr_callback = callbacks.LearningRateScheduler(scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e68cac6-4538-4503-afe0-4e7f2005b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0a64e4e-9c14-4453-b31d-cc5a977f34e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 463ms/step - accuracy: 0.5971 - loss: 2.2191 - val_accuracy: 0.4244 - val_loss: 2.9078 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 451ms/step - accuracy: 0.5994 - loss: 0.6703 - val_accuracy: 0.4244 - val_loss: 1.0801 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 452ms/step - accuracy: 0.6351 - loss: 0.6613 - val_accuracy: 0.5187 - val_loss: 0.7022 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 450ms/step - accuracy: 0.6533 - loss: 0.6416 - val_accuracy: 0.4971 - val_loss: 0.7097 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 454ms/step - accuracy: 0.6449 - loss: 0.6270 - val_accuracy: 0.5737 - val_loss: 0.6758 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 450ms/step - accuracy: 0.6707 - loss: 0.6116 - val_accuracy: 0.5855 - val_loss: 0.6581 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 453ms/step - accuracy: 0.7041 - loss: 0.5851 - val_accuracy: 0.5874 - val_loss: 0.8647 - learning_rate: 9.5000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 449ms/step - accuracy: 0.6714 - loss: 0.6134 - val_accuracy: 0.5658 - val_loss: 0.8539 - learning_rate: 9.0250e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 460ms/step - accuracy: 0.6695 - loss: 0.6140 - val_accuracy: 0.4538 - val_loss: 0.9594 - learning_rate: 8.5737e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 466ms/step - accuracy: 0.6753 - loss: 0.6274 - val_accuracy: 0.6189 - val_loss: 0.6898 - learning_rate: 8.1451e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 458ms/step - accuracy: 0.6881 - loss: 0.5829 - val_accuracy: 0.6542 - val_loss: 0.6224 - learning_rate: 7.7378e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 452ms/step - accuracy: 0.7130 - loss: 0.5888 - val_accuracy: 0.6916 - val_loss: 0.6021 - learning_rate: 7.3509e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 463ms/step - accuracy: 0.7051 - loss: 0.5760 - val_accuracy: 0.7053 - val_loss: 0.5768 - learning_rate: 6.9834e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 452ms/step - accuracy: 0.7163 - loss: 0.5764 - val_accuracy: 0.6935 - val_loss: 0.6534 - learning_rate: 6.6342e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 450ms/step - accuracy: 0.6881 - loss: 0.5791 - val_accuracy: 0.7092 - val_loss: 0.5615 - learning_rate: 6.3025e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 453ms/step - accuracy: 0.7017 - loss: 0.5691 - val_accuracy: 0.6739 - val_loss: 0.6946 - learning_rate: 5.9874e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 450ms/step - accuracy: 0.7126 - loss: 0.5614 - val_accuracy: 0.6582 - val_loss: 0.7698 - learning_rate: 5.6880e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 449ms/step - accuracy: 0.7303 - loss: 0.5429 - val_accuracy: 0.6169 - val_loss: 0.6919 - learning_rate: 5.4036e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 450ms/step - accuracy: 0.7209 - loss: 0.5470 - val_accuracy: 0.6071 - val_loss: 0.6839 - learning_rate: 5.1334e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 453ms/step - accuracy: 0.7133 - loss: 0.5422 - val_accuracy: 0.7230 - val_loss: 0.5529 - learning_rate: 4.8767e-04\n",
      "Epoch 21/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 452ms/step - accuracy: 0.7241 - loss: 0.5448 - val_accuracy: 0.6542 - val_loss: 0.6980 - learning_rate: 4.6329e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 452ms/step - accuracy: 0.7209 - loss: 0.5261 - val_accuracy: 0.7466 - val_loss: 0.5282 - learning_rate: 4.4013e-04\n",
      "Epoch 23/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 454ms/step - accuracy: 0.7288 - loss: 0.5296 - val_accuracy: 0.7466 - val_loss: 0.5235 - learning_rate: 4.1812e-04\n",
      "Epoch 24/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 449ms/step - accuracy: 0.7425 - loss: 0.5199 - val_accuracy: 0.7446 - val_loss: 0.5499 - learning_rate: 3.9721e-04\n",
      "Epoch 25/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 465ms/step - accuracy: 0.7530 - loss: 0.5088 - val_accuracy: 0.7328 - val_loss: 0.5435 - learning_rate: 3.7735e-04\n",
      "Epoch 26/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 499ms/step - accuracy: 0.7618 - loss: 0.5080 - val_accuracy: 0.6287 - val_loss: 0.6860 - learning_rate: 3.5849e-04\n",
      "Epoch 27/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 452ms/step - accuracy: 0.7704 - loss: 0.4814 - val_accuracy: 0.7544 - val_loss: 0.5166 - learning_rate: 3.4056e-04\n",
      "Epoch 28/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 466ms/step - accuracy: 0.7526 - loss: 0.5102 - val_accuracy: 0.7564 - val_loss: 0.5042 - learning_rate: 3.2353e-04\n",
      "Epoch 29/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 457ms/step - accuracy: 0.7521 - loss: 0.5018 - val_accuracy: 0.7603 - val_loss: 0.5320 - learning_rate: 3.0736e-04\n",
      "Epoch 30/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 464ms/step - accuracy: 0.7714 - loss: 0.4777 - val_accuracy: 0.7505 - val_loss: 0.5280 - learning_rate: 2.9199e-04\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 30\n",
    "history = model.fit(data_gen.flow(X_train, y_train, batch_size=32), \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    epochs=epochs, \n",
    "                    callbacks=[lr_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4ea0262-a4e1-40bb-8212-04c51090c82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training completed and saved as 'autism_cnn_model2.pkl'\n"
     ]
    }
   ],
   "source": [
    " #Save the trained model as a pickle file\n",
    "with open(\"autism_cnn_model2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "print(\"Model training completed and saved as 'autism_cnn_model2.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5182260-1d17-4cd2-b888-c9fb5feff44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "with open(\"autism_cnn_model2.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ad6e207-2f9a-457b-86e5-f8162425a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tkinter GUI for Image Upload\n",
    "def predict_image():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if not file_path:\n",
    "        return\n",
    "    \n",
    "    img = cv2.imread(file_path, cv2.IMREAD_COLOR)\n",
    "    img = cv2.resize(img, (img_size, img_size))\n",
    "    img = np.expand_dims(img, axis=0) / 255.0  # Normalize and reshape\n",
    "    prediction = model.predict(img)\n",
    "    class_index = np.argmax(prediction)\n",
    "    result_label.config(text=f\"Prediction: {categories[class_index]}\")\n",
    "    \n",
    "     # Display selected image\n",
    "    img = Image.open(file_path)\n",
    "    img = img.resize((200, 200))\n",
    "    img = ImageTk.PhotoImage(img)\n",
    "    panel.config(image=img)\n",
    "    panel.image = img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0812c6a-1e81-4ee5-8bad-04436d782f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n"
     ]
    }
   ],
   "source": [
    "# GUI Setup\n",
    "root = tk.Tk()\n",
    "root.title(\"Autism Detection Model\")\n",
    "root.geometry(\"400x400\")\n",
    "\n",
    "title_label = tk.Label(root, text=\"Autism Detection\", font=(\"Arial\", 16))\n",
    "title_label.pack(pady=10)\n",
    "\n",
    "upload_button = tk.Button(root, text=\"Upload Image\", command=predict_image)\n",
    "upload_button.pack(pady=10)\n",
    "\n",
    "result_label = tk.Label(root, text=\"\", font=(\"Arial\", 12))\n",
    "result_label.pack(pady=10)\n",
    "\n",
    "panel = tk.Label(root)\n",
    "panel.pack()\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6457914-2a24-411f-9069-cc4aa448d681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
